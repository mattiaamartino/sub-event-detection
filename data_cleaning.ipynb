{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "051a794f-c516-434a-bcca-7a8429f07e80",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import re\n",
    "from tqdm import tqdm\n",
    "\n",
    "def transform_hashtags(text):\n",
    "\n",
    "    \"\"\"Transforms hashtags into readable text.\"\"\"\n",
    "    hashtags = re.findall(r'#(\\w+)', text)\n",
    "    \n",
    "    for hashtag in hashtags:\n",
    "        words = hashtag.split('_')\n",
    "        separated_words = []\n",
    "        for word in words:\n",
    "            if word.isupper():\n",
    "                separated_words.append(word)\n",
    "            else:\n",
    "                split_words = re.findall('[A-Z][^A-Z]*', word)\n",
    "                if split_words:\n",
    "                    separated_words.extend(split_words)\n",
    "                else:\n",
    "                    separated_words.append(word)\n",
    "        clean_text = ' '.join(separated_words).lower()\n",
    "        text = text.replace(f'#{hashtag}', clean_text)\n",
    "    return text\n",
    "    \n",
    "def clean_and_process_tweets(input_folder, output_folder):\n",
    "    os.makedirs(output_folder, exist_ok=True) #check folder \n",
    "    phrases_to_remove = [\n",
    "        r'follow & rt to enter!?\\.?', \n",
    "        r'rt & follow to enter!?\\.?' \n",
    "    ]\n",
    "    \n",
    "    for file_name in tqdm(os.listdir(input_folder), desc='Cleaning and processing tweets', unit='files'):\n",
    "        if file_name.endswith('.csv'):\n",
    "            input_file_path = os.path.join(input_folder, file_name)\n",
    "            output_file_path = os.path.join(output_folder, file_name)\n",
    "            df = pd.read_csv(input_file_path)\n",
    "            duplicate_rows = df[df.duplicated()]\n",
    "            df = df.drop_duplicates() # drop duplicates\n",
    "            df = df[~df[\"Tweet\"].str.startswith(\"RT\")]\n",
    "            df = df[~df[\"Tweet\"].str.contains(\"@\")] \n",
    "            \n",
    "            if 'Tweet' in df.columns:\n",
    "                df = df.drop_duplicates(subset='Tweet') # drop duplicates in Tweets  \n",
    "                df['Tweet'] = df['Tweet'].apply(lambda x: re.sub(r'http\\S+|www\\S+|https\\S+|htt…', '', str(x), flags=re.MULTILINE)) # remove links \n",
    "                df['Tweet'] = df['Tweet'].apply(lambda x: re.sub(r'^RT\\s+@\\w+:\\s+', '', str(x))) # remove RT and usernames after that\n",
    "                df['Tweet'] = df['Tweet'].apply(lambda x: re.sub(r'@\\w+', 'user', str(x)))\n",
    "                df['Tweet'] = df['Tweet'].apply(transform_hashtags) # transform hashtags \n",
    "                df['Tweet'] = df['Tweet'].apply(lambda x: re.sub(r'#', '', str(x))) # remove leftover hashtags\n",
    "                df['Tweet'] = df['Tweet'].apply(lambda x: str(x).lower()) # convert to lower case\n",
    "                for phrase in phrases_to_remove:\n",
    "                    df['Tweet'] = df['Tweet'].apply(lambda x: re.sub(phrase, '', str(x), flags=re.IGNORECASE)) # delete \"follow & rt to enter!\" or \"rt & follow to enter.\"\n",
    "                df['Tweet'] = df['Tweet'].apply(lambda x: re.sub(r'^:\\s*', '', str(x))) # delete colons from beginning of rows\n",
    "                df['Tweet'] = df['Tweet'].apply(lambda x: re.sub(r'\\s+', ' ', str(x)))  # remove extra spaces\n",
    "                df['Tweet'] = df['Tweet'].apply(lambda x: x.strip()) # remove leading and trailing spaces\n",
    "                df['Tweet'] = df['Tweet'].apply(lambda x: x.replace('\\n', ' ')) # replace newlines with spaces\n",
    "            \n",
    "            df.to_csv(output_file_path, index=False) # save dataset \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "203eab50-24fa-4aaa-98a4-947a7539103c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Cleaning and processing tweets: 100%|██████████| 16/16 [00:35<00:00,  2.25s/files]\n"
     ]
    }
   ],
   "source": [
    "clean_and_process_tweets('challenge_data/train_tweets', 'cleaned_data/train_data')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "47a68b57",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Cleaning and processing tweets: 100%|██████████| 4/4 [00:07<00:00,  1.90s/files]\n"
     ]
    }
   ],
   "source": [
    "clean_and_process_tweets('challenge_data/eval_tweets', 'cleaned_data/eval_data')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b9772ea4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Cleaning and processing tweets:   0%|          | 0/16 [00:00<?, ?files/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Cleaning and processing tweets: 100%|██████████| 16/16 [00:26<00:00,  1.67s/files]\n"
     ]
    }
   ],
   "source": [
    "clean_and_process_tweets('challenge_data/train_tweets', 'cleaned_data/final_approach/test_data')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e9e0eba2",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Cleaning and processing tweets: 100%|██████████| 4/4 [00:06<00:00,  1.51s/files]\n"
     ]
    }
   ],
   "source": [
    "clean_and_process_tweets('challenge_data/eval_tweets', 'cleaned_data/final_approach/eval_data')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nlp_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
