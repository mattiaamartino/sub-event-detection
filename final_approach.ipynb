{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 1.: Embedding each tweet in the time periods"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils.preprocess_dataset import pre_processing_feature_extraction\n",
    "from utils.finetuned_embedding import get_pre_classifier_output\n",
    "from tqdm import tqdm\n",
    "import pandas as pd\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tqdm.pandas()\n",
    "\n",
    "files = os.listdir('cleaned_data/final_approach/train_data')\n",
    "for i, filename in enumerate(files):\n",
    "    print(f\"Processing {filename} ({i+1}/{len(files)})\")\n",
    "    if filename.endswith('.csv'):\n",
    "        df = pre_processing_feature_extraction(f'cleaned_data/final_approach/train_data/{filename}', \"train\")\n",
    "        df[\"Embeddings\"] = df[\"Tweet\"].progress_apply(get_pre_classifier_output)\n",
    "        filename = filename.replace('.csv', '.pkl')\n",
    "        df[[\"ID\", \"Embeddings\", \"EventType\"]].to_pickle(f'processed_data/train_data/{filename}')\n",
    "        print(f\"Saved {filename} to processed_data/train_data\")\n",
    "        del df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "files = os.listdir('cleaned_data/final_approach/eval_data')\n",
    "for i, filename in enumerate(files):\n",
    "    print(f\"Processing {filename} ({i+1}/{len(files)})\")\n",
    "    if filename.endswith('.csv'):\n",
    "        df = pre_processing_feature_extraction(f'cleaned_data/final_approach/eval_data/{filename}', mode='eval')\n",
    "        df[\"Embeddings\"] = df[\"Tweet\"].progress_apply(get_pre_classifier_output)\n",
    "        filename = filename.replace('.csv', '.pkl')\n",
    "        df[[\"ID\", \"Embeddings\"]].to_pickle(f'processed_data/eval_data/{filename}')\n",
    "        print(f\"Saved {filename} to processed_data/eval_data\")\n",
    "        del df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 2.: Performing K-Means for each time period on the embedded tweets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd \n",
    "from tqdm import tqdm\n",
    "from sklearn.cluster import KMeans, HDBSCAN\n",
    "import numpy as np\n",
    "import os\n",
    "\n",
    "def cluster_embeddings(embeddings, n_clusters = 9, standardize = False):\n",
    "    \n",
    "    if standardize:\n",
    "        mean = embeddings.mean(axis=0)\n",
    "        std = embeddings.std(axis=0)\n",
    "        embeddings = (embeddings - mean) / std\n",
    "\n",
    "    kmeans = KMeans(n_clusters=n_clusters, random_state=0).fit(embeddings)\n",
    "    centroids_count = np.bincount(kmeans.labels_)\n",
    "    return kmeans.cluster_centers_, centroids_count\n",
    "\n",
    "def cluster_embeddings_hdbscan(embeddings, min_cluster_size = 5):\n",
    "    clusterer = HDBSCAN(min_cluster_size=min_cluster_size, store_centers=\"medoid\").fit(embeddings)\n",
    "    labels = clusterer.labels_\n",
    "    medoids_count = np.bincount(labels[labels >= 0])\n",
    "    return clusterer.medoids_, medoids_count\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 16/16 [00:01<00:00,  8.79it/s]\n"
     ]
    }
   ],
   "source": [
    "train_data = pd.DataFrame([])\n",
    "\n",
    "for file in tqdm(os.listdir('processed_data/train_data')):\n",
    "    if file.endswith('.pkl'):\n",
    "        df = pd.read_pickle(f'processed_data/train_data/{file}')\n",
    "        train_data = pd.concat([train_data, df], ignore_index=True)\n",
    "        del df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2137/2137 [45:20<00:00,  1.27s/it]  \n"
     ]
    }
   ],
   "source": [
    "tqdm.pandas()\n",
    "train_data[\"Clusters\"] = train_data[\"Embeddings\"].progress_apply(cluster_embeddings_hdbscan)\n",
    "train_data.drop(columns=[\"Embeddings\"], inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/alessandropranzo/.pyenv/versions/3.10.15/envs/nlp_env/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from scipy.spatial.distance import euclidean\n",
    "import torch\n",
    "from torch_geometric.data import Data\n",
    "\n",
    "def construct_graph_for_gnn(input, normalize_weights=False):\n",
    "    \"\"\"\n",
    "    Constructs a graph from centroids and cluster sizes for use in a Graph Neural Network.\n",
    "\n",
    "    Parameters:\n",
    "    centroids (np.ndarray): Array of centroid coordinates (n_clusters, embedding_dim).\n",
    "    cluster_sizes (list): List of cluster sizes (n_clusters).\n",
    "\n",
    "    Returns:\n",
    "    torch_geometric.data.Data: A graph data object with node features, edge index, and edge weights.\n",
    "    \"\"\"\n",
    "    centroids, cluster_sizes = input\n",
    "    n = len(centroids)\n",
    "    edges = []\n",
    "    raw_weights = []\n",
    "\n",
    "    if not normalize_weights:\n",
    "        cluster_sizes = cluster_sizes / sum(cluster_sizes)\n",
    "\n",
    "    # Step 1: Compute raw weights and edges\n",
    "    for i in range(n):\n",
    "        for j in range(i + 1, n):\n",
    "            distance = euclidean(centroids[i], centroids[j])\n",
    "            weight = (cluster_sizes[i] + cluster_sizes[j]) / (1 + distance)  # Raw weight\n",
    "            edges.append((i, j))\n",
    "            raw_weights.append(weight)\n",
    "\n",
    "    # Step 2: Normalize weights\n",
    "    if normalize_weights:\n",
    "        raw_weights = torch.tensor(raw_weights, dtype=torch.float)\n",
    "        min_weight = raw_weights.min()\n",
    "        max_weight = raw_weights.max()\n",
    "        normalized_weights = (raw_weights - min_weight) / (max_weight - min_weight)\n",
    "    else:\n",
    "        normalized_weights = torch.tensor(raw_weights, dtype=torch.float)\n",
    "\n",
    "    # Step 3: Prepare edge index and edge attributes\n",
    "    edge_index = torch.tensor(edges, dtype=torch.long).t().contiguous()  # Edge index tensor\n",
    "    edge_attr = normalized_weights  # Normalized weights as edge attributes\n",
    "\n",
    "    # Step 4: Prepare node features\n",
    "    x = torch.tensor(centroids, dtype=torch.float)  # Node features (centroids)\n",
    "\n",
    "    # Step 5: Create PyTorch Geometric Data object\n",
    "    graph = Data(x=x, edge_index=edge_index, edge_attr=edge_attr)\n",
    "\n",
    "    return graph"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 3.: Train the Graph Neural Network on the new representation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch_geometric.loader import DataLoader\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "def create_dataloaders(data, labels, batch_size=16, test_size=0.2, random_state=42):\n",
    "    \"\"\"\n",
    "    Creates PyTorch Geometric DataLoaders from a dataset.\n",
    "\n",
    "    Parameters:\n",
    "    - data (list of tuples): Each tuple contains (centroids, cluster_sizes).\n",
    "    - labels (list of int): Corresponding binary labels (0 or 1) for each graph.\n",
    "    - batch_size (int): Batch size for the DataLoader.\n",
    "    - test_size (float): Fraction of data to be used for validation.\n",
    "    - random_state (int): Seed for reproducibility.\n",
    "\n",
    "    Returns:\n",
    "    - train_loader (DataLoader): DataLoader for the training set.\n",
    "    - val_loader (DataLoader): DataLoader for the validation set.\n",
    "    \"\"\"\n",
    "    graphs = []\n",
    "    \n",
    "    # Create graphs using the construct_graph_for_gnn function\n",
    "    for input_data, label in zip(data, labels):\n",
    "        graph = construct_graph_for_gnn(input_data)\n",
    "        graph.y = torch.tensor([label], dtype=torch.float)  # Add label to the graph\n",
    "        graphs.append(graph)\n",
    "    \n",
    "    # Split dataset into training and validation sets\n",
    "    train_graphs, val_graphs = train_test_split(graphs, test_size=test_size, random_state=random_state)\n",
    "    \n",
    "    # Create DataLoaders\n",
    "    train_loader = DataLoader(train_graphs, batch_size=batch_size, shuffle=True)\n",
    "    val_loader = DataLoader(val_graphs, batch_size=batch_size)\n",
    "    \n",
    "    return train_loader, val_loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from torch_geometric.nn import GCNConv, global_mean_pool\n",
    "from torch.nn import BCEWithLogitsLoss\n",
    "from torch.optim import Adam\n",
    "from tqdm import tqdm\n",
    "\n",
    "class GraphClassifier(torch.nn.Module):\n",
    "    def __init__(self, in_channels, hidden_channels, num_classes):\n",
    "        super(GraphClassifier, self).__init__()\n",
    "        self.conv1 = GCNConv(in_channels, hidden_channels)\n",
    "        self.conv2 = GCNConv(hidden_channels, hidden_channels)\n",
    "        self.conv3 = GCNConv(hidden_channels, hidden_channels)\n",
    "        self.fc = torch.nn.Linear(hidden_channels, 1)  # Change to single output for binary classification\n",
    "\n",
    "    def forward(self, data):\n",
    "        # Input graph data\n",
    "        x, edge_index, edge_attr, batch = data.x, data.edge_index, data.edge_attr, data.batch\n",
    "        \n",
    "        # Graph convolution layers\n",
    "        x = self.conv1(x, edge_index, edge_weight=edge_attr)\n",
    "        x = F.relu(x)\n",
    "        x = self.conv2(x, edge_index, edge_weight=edge_attr)\n",
    "        x = F.relu(x)\n",
    "        x = self.conv3(x, edge_index, edge_weight=edge_attr)\n",
    "        x = F.relu(x)\n",
    "\n",
    "        # Global pooling\n",
    "        x = global_mean_pool(x, batch)  # Aggregate node embeddings\n",
    "\n",
    "        # Fully connected layer for classification\n",
    "        x = self.fc(x)\n",
    "        return x  # Binary classification output\n",
    "\n",
    "\n",
    "def train(model, train_loader, val_loader, epochs, learning_rate):\n",
    "    \"\"\"\n",
    "    Train a GNN model for binary graph classification on an MPS-based machine.\n",
    "\n",
    "    Parameters:\n",
    "    - model (torch.nn.Module): The GNN model to train.\n",
    "    - train_loader (DataLoader): DataLoader for training data.\n",
    "    - val_loader (DataLoader): DataLoader for validation data.\n",
    "    - epochs (int): Number of epochs to train.\n",
    "    - learning_rate (float): Learning rate for the optimizer.\n",
    "\n",
    "    Returns:\n",
    "    - model (torch.nn.Module): Trained model.\n",
    "    \"\"\"\n",
    "    # Device configuration for MPS\n",
    "    device = torch.device(\"mps\") if torch.backends.mps.is_available() else torch.device(\"cpu\")\n",
    "    model.to(device)\n",
    "\n",
    "    # Loss and optimizer\n",
    "    criterion = BCEWithLogitsLoss()\n",
    "    optimizer = Adam(model.parameters(), lr=learning_rate)\n",
    "\n",
    "    training_losses = []\n",
    "    validation_losses = []\n",
    "    training_accuracies = []\n",
    "    validation_accuracies = []\n",
    "\n",
    "    # Training loop\n",
    "    for i, epoch in tqdm(enumerate(range(epochs)), desc=\"Epochs\", unit=\"epoch\"):\n",
    "        model.train()\n",
    "        train_loss = 0\n",
    "        correct = 0\n",
    "        total = 0\n",
    "\n",
    "        for batch in train_loader:\n",
    "            batch = batch.to(device)\n",
    "            \n",
    "            optimizer.zero_grad()\n",
    "            # Forward pass\n",
    "            out = model(batch).squeeze()  # Ensure output is flattened\n",
    "            loss = criterion(out, batch.y.float())  # Match tensor dimensions\n",
    "            \n",
    "            # Backward pass and optimization\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "            train_loss += loss.item()\n",
    "            \n",
    "            # Calculate training accuracy\n",
    "            preds = torch.sigmoid(out) > 0.5\n",
    "            correct += (preds == batch.y).sum().item()\n",
    "            total += batch.y.size(0)\n",
    "\n",
    "        train_loss /= len(train_loader)\n",
    "        train_accuracy = correct / total\n",
    "\n",
    "        training_losses.append(train_loss)\n",
    "        training_accuracies.append(train_accuracy)\n",
    "        \n",
    "        # Validation loop\n",
    "        model.eval()\n",
    "        val_loss = 0\n",
    "        correct = 0\n",
    "        total = 0\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            for batch in val_loader:\n",
    "                batch = batch.to(device)\n",
    "                out = model(batch).squeeze()  # Ensure output is flattened\n",
    "                loss = criterion(out, batch.y.float())\n",
    "                val_loss += loss.item()\n",
    "                \n",
    "                # Calculate validation accuracy\n",
    "                preds = torch.sigmoid(out) > 0.5\n",
    "                correct += (preds == batch.y).sum().item()\n",
    "                total += batch.y.size(0)\n",
    "\n",
    "        val_loss /= len(val_loader)\n",
    "        val_accuracy = correct / total\n",
    "\n",
    "        validation_losses.append(val_loss)\n",
    "        validation_accuracies.append(val_accuracy)\n",
    "\n",
    "        if i % 10 == 0:\n",
    "            print(f\"Epoch {epoch + 1}/{epochs}: Train Loss = {train_loss:.4f}, Train Accuracy = {train_accuracy:.4f}, Val Loss = {val_loss:.4f}, Val Accuracy = {val_accuracy:.4f}\")\n",
    "\n",
    "    return model, training_losses, validation_losses, training_accuracies, validation_accuracies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import torch\n",
    "# import torch.nn.functional as F\n",
    "# from torch_geometric.nn import GCNConv, global_mean_pool\n",
    "# from torch.nn import BCEWithLogitsLoss\n",
    "# from torch.optim import Adam\n",
    "# from tqdm import tqdm\n",
    "\n",
    "# class GraphClassifier(torch.nn.Module):\n",
    "#     def __init__(self, in_channels, hidden_channels, num_classes):\n",
    "#         super(GraphClassifier, self).__init__()\n",
    "#         self.conv1 = GCNConv(in_channels, hidden_channels)\n",
    "#         self.conv2 = GCNConv(hidden_channels, hidden_channels)\n",
    "#         self.conv3 = GCNConv(hidden_channels, hidden_channels)\n",
    "#         self.fc = torch.nn.Linear(hidden_channels, num_classes)\n",
    "\n",
    "#     def forward(self, data):\n",
    "#         # Input graph data\n",
    "#         x, edge_index, edge_attr, batch = data.x, data.edge_index, data.edge_attr, data.batch\n",
    "        \n",
    "#         # Graph convolution layers\n",
    "#         x = self.conv1(x, edge_index, edge_weight=edge_attr)\n",
    "#         x = F.relu(x)\n",
    "#         x = self.conv2(x, edge_index, edge_weight=edge_attr)\n",
    "#         x = F.relu(x)\n",
    "#         x = self.conv3(x, edge_index, edge_weight=edge_attr)\n",
    "#         x = F.relu(x)\n",
    "\n",
    "#         # Global pooling\n",
    "#         x = global_mean_pool(x, batch)  # Aggregate node embeddings\n",
    "\n",
    "#         # Fully connected layer for classification\n",
    "#         x = self.fc(x)\n",
    "#         return x  # Binary classification\n",
    "\n",
    "\n",
    "# def train(model, train_loader, val_loader, epochs, learning_rate):\n",
    "#     \"\"\"\n",
    "#     Train a GNN model for binary graph classification on an MPS-based machine.\n",
    "\n",
    "#     Parameters:\n",
    "#     - model (torch.nn.Module): The GNN model to train.\n",
    "#     - train_loader (DataLoader): DataLoader for training data.\n",
    "#     - val_loader (DataLoader): DataLoader for validation data.\n",
    "#     - epochs (int): Number of epochs to train.\n",
    "#     - learning_rate (float): Learning rate for the optimizer.\n",
    "\n",
    "#     Returns:\n",
    "#     - model (torch.nn.Module): Trained model.\n",
    "#     \"\"\"\n",
    "#     # Device configuration for MPS\n",
    "#     #device = torch.device(\"cpu\")\n",
    "#     device = torch.device(\"mps\") if torch.backends.mps.is_available() else torch.device(\"cpu\")\n",
    "#     model.to(device)\n",
    "\n",
    "#     # Loss and optimizer\n",
    "#     criterion = BCEWithLogitsLoss()\n",
    "#     optimizer = Adam(model.parameters(), lr=learning_rate)\n",
    "\n",
    "#     training_losses = []\n",
    "#     validation_losses = []\n",
    "#     training_accuracies = []\n",
    "#     validation_accuracies = []\n",
    "\n",
    "#     # Training loop\n",
    "#     for i, epoch in tqdm(enumerate(range(epochs)), desc=\"Epochs\", unit=\"epoch\"):\n",
    "#         model.train()\n",
    "#         train_loss = 0\n",
    "#         correct = 0\n",
    "#         total = 0\n",
    "\n",
    "#         for batch in train_loader:\n",
    "#             batch = batch.to(device)\n",
    "            \n",
    "#             optimizer.zero_grad()\n",
    "#             # Forward pass\n",
    "#             out = model(batch)\n",
    "#             loss = criterion(out.squeeze(), batch.y.float())  # Squeeze for binary labels\n",
    "            \n",
    "#             # Backward pass and optimization\n",
    "#             loss.backward()\n",
    "#             optimizer.step()\n",
    "            \n",
    "#             train_loss += loss.item()\n",
    "            \n",
    "#             # Calculate training accuracy\n",
    "#             preds = torch.sigmoid(out).squeeze() > 0.5\n",
    "#             correct += (preds == batch.y).sum().item()\n",
    "#             total += batch.y.size(0)\n",
    "\n",
    "#         train_loss /= len(train_loader)\n",
    "#         train_accuracy = correct / total\n",
    "\n",
    "#         training_losses.append(train_loss)\n",
    "#         training_accuracies.append(train_accuracy)\n",
    "        \n",
    "#         # Validation loop\n",
    "#         model.eval()\n",
    "#         val_loss = 0\n",
    "#         correct = 0\n",
    "#         total = 0\n",
    "        \n",
    "#         with torch.no_grad():\n",
    "#             for batch in val_loader:\n",
    "#                 batch = batch.to(device)\n",
    "#                 out = model(batch)\n",
    "#                 loss = criterion(out.squeeze(), batch.y.float())\n",
    "#                 val_loss += loss.item()\n",
    "                \n",
    "#                 # Calculate validation accuracy\n",
    "#                 preds = torch.sigmoid(out).squeeze() > 0.5\n",
    "#                 correct += (preds == batch.y).sum().item()\n",
    "#                 total += batch.y.size(0)\n",
    "\n",
    "#         val_loss /= len(val_loader)\n",
    "#         val_accuracy = correct / total\n",
    "\n",
    "#         validation_losses.append(val_loss)\n",
    "#         validation_accuracies.append(val_accuracy)\n",
    "\n",
    "#         if i % 10 == 0:\n",
    "#             print(f\"Epoch {epoch + 1}/{epochs}: Train Loss = {train_loss:.4f}, Train Accuracy = {train_accuracy:.4f}, Val Loss = {val_loss:.4f}, Val Accuracy = {val_accuracy:.4f}\")\n",
    "\n",
    "#     return model, training_losses, validation_losses, training_accuracies, validation_accuracies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize model\n",
    "model = GraphClassifier(in_channels=768, hidden_channels=256, num_classes=1)\n",
    "\n",
    "train_loader, val_loader = create_dataloaders(train_data[\"Clusters\"], train_data[\"EventType\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epochs: 0epoch [00:00, ?epoch/s]\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Target size (torch.Size([16])) must be the same as input size (torch.Size([15]))",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[16], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m trained_model, training_losses, validation_losses, training_accuracies, validation_accuracies \u001b[38;5;241m=\u001b[39m \u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mval_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mepochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m50\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlearning_rate\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m0.01\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[13], line 76\u001b[0m, in \u001b[0;36mtrain\u001b[0;34m(model, train_loader, val_loader, epochs, learning_rate)\u001b[0m\n\u001b[1;32m     74\u001b[0m \u001b[38;5;66;03m# Forward pass\u001b[39;00m\n\u001b[1;32m     75\u001b[0m out \u001b[38;5;241m=\u001b[39m model(batch)\u001b[38;5;241m.\u001b[39msqueeze()  \u001b[38;5;66;03m# Ensure output is flattened\u001b[39;00m\n\u001b[0;32m---> 76\u001b[0m loss \u001b[38;5;241m=\u001b[39m \u001b[43mcriterion\u001b[49m\u001b[43m(\u001b[49m\u001b[43mout\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbatch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43my\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfloat\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# Match tensor dimensions\u001b[39;00m\n\u001b[1;32m     78\u001b[0m \u001b[38;5;66;03m# Backward pass and optimization\u001b[39;00m\n\u001b[1;32m     79\u001b[0m loss\u001b[38;5;241m.\u001b[39mbackward()\n",
      "File \u001b[0;32m~/.pyenv/versions/3.10.15/envs/nlp_env/lib/python3.10/site-packages/torch/nn/modules/module.py:1736\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1734\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1735\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1736\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.pyenv/versions/3.10.15/envs/nlp_env/lib/python3.10/site-packages/torch/nn/modules/module.py:1747\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1742\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1743\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1744\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1745\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1746\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1747\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1749\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1750\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[0;32m~/.pyenv/versions/3.10.15/envs/nlp_env/lib/python3.10/site-packages/torch/nn/modules/loss.py:819\u001b[0m, in \u001b[0;36mBCEWithLogitsLoss.forward\u001b[0;34m(self, input, target)\u001b[0m\n\u001b[1;32m    818\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor, target: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[0;32m--> 819\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbinary_cross_entropy_with_logits\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    820\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m    821\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtarget\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    822\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    823\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpos_weight\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpos_weight\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    824\u001b[0m \u001b[43m        \u001b[49m\u001b[43mreduction\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mreduction\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    825\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.pyenv/versions/3.10.15/envs/nlp_env/lib/python3.10/site-packages/torch/nn/functional.py:3624\u001b[0m, in \u001b[0;36mbinary_cross_entropy_with_logits\u001b[0;34m(input, target, weight, size_average, reduce, reduction, pos_weight)\u001b[0m\n\u001b[1;32m   3621\u001b[0m     reduction_enum \u001b[38;5;241m=\u001b[39m _Reduction\u001b[38;5;241m.\u001b[39mget_enum(reduction)\n\u001b[1;32m   3623\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (target\u001b[38;5;241m.\u001b[39msize() \u001b[38;5;241m==\u001b[39m \u001b[38;5;28minput\u001b[39m\u001b[38;5;241m.\u001b[39msize()):\n\u001b[0;32m-> 3624\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m   3625\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTarget size (\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtarget\u001b[38;5;241m.\u001b[39msize()\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m) must be the same as input size (\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28minput\u001b[39m\u001b[38;5;241m.\u001b[39msize()\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m)\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   3626\u001b[0m     )\n\u001b[1;32m   3628\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mbinary_cross_entropy_with_logits(\n\u001b[1;32m   3629\u001b[0m     \u001b[38;5;28minput\u001b[39m, target, weight, pos_weight, reduction_enum\n\u001b[1;32m   3630\u001b[0m )\n",
      "\u001b[0;31mValueError\u001b[0m: Target size (torch.Size([16])) must be the same as input size (torch.Size([15]))"
     ]
    }
   ],
   "source": [
    "trained_model, training_losses, validation_losses, training_accuracies, validation_accuracies = train(model, train_loader, val_loader, epochs=50, learning_rate=0.01)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Plot training and validation losses\n",
    "plt.figure(figsize=(12, 6))\n",
    "\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.plot(training_losses, label='Training Loss')\n",
    "plt.plot(validation_losses, label='Validation Loss')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Loss')\n",
    "plt.title('Training and Validation Losses')\n",
    "plt.legend()\n",
    "\n",
    "# Plot training and validation accuracies\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.plot(training_accuracies, label='Training Accuracy')\n",
    "plt.plot(validation_accuracies, label='Validation Accuracy')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.title('Training and Validation Accuracies')\n",
    "plt.legend()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 4.: Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def infer(model, data_loader, threshold=0.5, device=None):\n",
    "    \"\"\"\n",
    "    Perform inference using the trained GraphClassifier model.\n",
    "\n",
    "    Parameters:\n",
    "    - model (torch.nn.Module): Trained GraphClassifier model.\n",
    "    - data_loader (torch.utils.data.DataLoader): DataLoader containing the test or inference data.\n",
    "    - threshold (float): Threshold for binary classification. Default is 0.5.\n",
    "    - device (torch.device): Device to run inference on. Defaults to 'cpu'.\n",
    "\n",
    "    Returns:\n",
    "    - predictions (list): Predicted binary labels for each graph.\n",
    "    - probabilities (list): Predicted probabilities for each graph.\n",
    "    \"\"\"\n",
    "    if device is None:\n",
    "        device = torch.device(\"cpu\")\n",
    "\n",
    "    model.eval()  # Set model to evaluation mode\n",
    "    model.to(device)\n",
    "\n",
    "    predictions = []\n",
    "\n",
    "    with torch.no_grad():  # Disable gradient computation for inference\n",
    "        for batch in data_loader:\n",
    "            batch = batch.to(device)\n",
    "            logits = model(batch)  # Forward pass\n",
    "            probs = torch.sigmoid(logits).squeeze()  # Apply sigmoid for probabilities\n",
    "\n",
    "            # Convert probabilities to binary predictions\n",
    "            preds = (probs > threshold).long()\n",
    "\n",
    "\n",
    "            predictions.extend(preds.cpu().numpy())\n",
    "\n",
    "    return predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "eval_data = pd.DataFrame([])\n",
    "\n",
    "for file in tqdm(os.listdir('processed_data/eval_data')):\n",
    "    if file.endswith('.pkl'):\n",
    "        df = pd.read_pickle(f'processed_data/eval_data/{file}')\n",
    "        eval_data = pd.concat([eval_data, df], ignore_index=True)\n",
    "        del df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tqdm.pandas()\n",
    "eval_data[\"Clusters\"] = eval_data[\"Embeddings\"].progress_apply(cluster_embeddings)\n",
    "eval_data.drop(columns=[\"Embeddings\"], inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "eval_loader = DataLoader([construct_graph_for_gnn(item) for item in eval_data[\"Clusters\"].values], batch_size=16)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = infer(trained_model, eval_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "submission = pd.DataFrame({\"ID\": eval_data[\"ID\"], \"EventType\": predictions})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "submission.to_csv(\"model_output/submissions/sub_4/submission.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nlp_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
