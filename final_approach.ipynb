{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 1.: Embedding each tweet in the time periods"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils.preprocess_dataset import pre_processing_feature_extraction\n",
    "from utils.finetuned_embedding import get_pre_classifier_output\n",
    "from tqdm import tqdm\n",
    "import pandas as pd\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tqdm.pandas()\n",
    "\n",
    "files = os.listdir('cleaned_data/final_approach/train_data')\n",
    "for i, filename in enumerate(files):\n",
    "    print(f\"Processing {filename} ({i+1}/{len(files)})\")\n",
    "    if filename.endswith('.csv'):\n",
    "        df = pre_processing_feature_extraction(f'cleaned_data/final_approach/train_data/{filename}', \"train\")\n",
    "        df[\"Embeddings\"] = df[\"Tweet\"].progress_apply(get_pre_classifier_output)\n",
    "        filename = filename.replace('.csv', '.pkl')\n",
    "        df[[\"ID\", \"Embeddings\", \"EventType\"]].to_pickle(f'processed_data/train_data/{filename}')\n",
    "        print(f\"Saved {filename} to processed_data/train_data\")\n",
    "        del df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "files = os.listdir('cleaned_data/final_approach/eval_data')\n",
    "for i, filename in enumerate(files):\n",
    "    print(f\"Processing {filename} ({i+1}/{len(files)})\")\n",
    "    if filename.endswith('.csv'):\n",
    "        df = pre_processing_feature_extraction(f'cleaned_data/final_approach/eval_data/{filename}', mode='eval')\n",
    "        df[\"Embeddings\"] = df[\"Tweet\"].progress_apply(get_pre_classifier_output)\n",
    "        filename = filename.replace('.csv', '.pkl')\n",
    "        df[[\"ID\", \"Embeddings\"]].to_pickle(f'processed_data/eval_data/{filename}')\n",
    "        print(f\"Saved {filename} to processed_data/eval_data\")\n",
    "        del df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 2.: Performing K-Means for each time period on the embedded tweets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd \n",
    "from tqdm import tqdm\n",
    "from sklearn.cluster import KMeans, HDBSCAN\n",
    "import numpy as np\n",
    "import os\n",
    "\n",
    "def cluster_embeddings(embeddings, n_clusters = 9, standardize = False):\n",
    "    \n",
    "    if standardize:\n",
    "        mean = embeddings.mean(axis=0)\n",
    "        std = embeddings.std(axis=0)\n",
    "        embeddings = (embeddings - mean) / std\n",
    "\n",
    "    kmeans = KMeans(n_clusters=n_clusters, random_state=0).fit(embeddings)\n",
    "    centroids_count = np.bincount(kmeans.labels_)\n",
    "    return kmeans.cluster_centers_, centroids_count\n",
    "\n",
    "def cluster_embeddings_hdbscan(embeddings, min_cluster_size = 5):\n",
    "    clusterer = HDBSCAN(min_cluster_size=min_cluster_size, store_centers=\"medoid\").fit(embeddings)\n",
    "    labels = clusterer.labels_\n",
    "    medoids_count = np.bincount(labels[labels >= 0])\n",
    "    return clusterer.medoids_, medoids_count\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = pd.DataFrame([])\n",
    "\n",
    "for file in tqdm(os.listdir('processed_data/train_data')):\n",
    "    if file.endswith('.pkl'):\n",
    "        df = pd.read_pickle(f'processed_data/train_data/{file}')\n",
    "        train_data = pd.concat([train_data, df], ignore_index=True)\n",
    "        del df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tqdm.pandas()\n",
    "train_data[\"Clusters\"] = train_data[\"Embeddings\"].progress_apply(cluster_embeddings_hdbscan)\n",
    "train_data.drop(columns=[\"Embeddings\"], inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from scipy.spatial.distance import euclidean\n",
    "# import torch\n",
    "# from torch_geometric.data import Data\n",
    "\n",
    "# def construct_graph_for_gnn(input, normalize_weights=False):\n",
    "#     \"\"\"\n",
    "#     Constructs a graph from centroids and cluster sizes for use in a Graph Neural Network.\n",
    "\n",
    "#     Parameters:\n",
    "#     centroids (np.ndarray): Array of centroid coordinates (n_clusters, embedding_dim).\n",
    "#     cluster_sizes (list): List of cluster sizes (n_clusters).\n",
    "\n",
    "#     Returns:\n",
    "#     torch_geometric.data.Data: A graph data object with node features, edge index, and edge weights.\n",
    "#     \"\"\"\n",
    "#     centroids, cluster_sizes = input\n",
    "#     n = len(centroids)\n",
    "#     edges = []\n",
    "#     raw_weights = []\n",
    "\n",
    "#     if n == 0:\n",
    "#         dummy_node = torch.zeros((1, centroids.shape[1]), torch.float)\n",
    "#         graph = Data(x=dummy_node, edge_index=torch.empty((2, 0), torch.long), edge_attr=torch.zeros(0, torch.float))\n",
    "#         return graph\n",
    "\n",
    "\n",
    "#     if not normalize_weights:\n",
    "#         cluster_sizes = cluster_sizes / sum(cluster_sizes)\n",
    "\n",
    "#     # Step 1: Compute raw weights and edges\n",
    "#     for i in range(n):\n",
    "#         for j in range(i + 1, n):\n",
    "#             distance = euclidean(centroids[i], centroids[j])\n",
    "#             weight = (cluster_sizes[i] + cluster_sizes[j]) / (1 + distance)  # Raw weight\n",
    "#             edges.append((i, j))\n",
    "#             raw_weights.append(weight)\n",
    "\n",
    "#     # Step 2: Normalize weights\n",
    "#     if normalize_weights:\n",
    "#         raw_weights = torch.tensor(raw_weights, dtype=torch.float)\n",
    "#         min_weight = raw_weights.min()\n",
    "#         max_weight = raw_weights.max()\n",
    "#         normalized_weights = (raw_weights - min_weight) / (max_weight - min_weight)\n",
    "#     else:\n",
    "#         normalized_weights = torch.tensor(raw_weights, dtype=torch.float)\n",
    "\n",
    "#     # Step 3: Prepare edge index and edge attributes\n",
    "#     edge_index = torch.tensor(edges, dtype=torch.long).t().contiguous()  # Edge index tensor\n",
    "#     edge_attr = normalized_weights  # Normalized weights as edge attributes\n",
    "\n",
    "#     # Step 4: Prepare node features\n",
    "#     x = torch.tensor(centroids, dtype=torch.float)  # Node features (centroids)\n",
    "\n",
    "#     # Step 5: Create PyTorch Geometric Data object\n",
    "#     graph = Data(x=x, edge_index=edge_index, edge_attr=edge_attr)\n",
    "\n",
    "#     return graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.spatial.distance import euclidean\n",
    "import torch\n",
    "from torch_geometric.data import Data\n",
    "\n",
    "def construct_graph_for_gnn(input, normalize_weights=False):\n",
    "    centroids, cluster_sizes = input\n",
    "\n",
    "    # If no clusters are found, create a dummy node with a zero vector.\n",
    "    # Assuming the original embeddings have dimension 768 (to match model input):\n",
    "    if len(centroids) == 0:\n",
    "        dummy_node = torch.zeros((1, 768), dtype=torch.float)\n",
    "        graph = Data(x=dummy_node, edge_index=torch.empty((2,0), dtype=torch.long), edge_attr=None)\n",
    "        return graph\n",
    "\n",
    "    if not normalize_weights:\n",
    "        cluster_sizes = cluster_sizes / sum(cluster_sizes)\n",
    "\n",
    "    n = len(centroids)\n",
    "    edges = []\n",
    "    raw_weights = []\n",
    "\n",
    "    # Construct the graph edges and weights\n",
    "    for i in range(n):\n",
    "        for j in range(i + 1, n):\n",
    "            distance = euclidean(centroids[i], centroids[j])\n",
    "            weight = (cluster_sizes[i] + cluster_sizes[j]) / (1 + distance)\n",
    "            edges.append((i, j))\n",
    "            raw_weights.append(weight)\n",
    "\n",
    "    # Normalize weights if required\n",
    "    if normalize_weights and len(raw_weights) > 0:\n",
    "        raw_weights = torch.tensor(raw_weights, dtype=torch.float)\n",
    "        min_weight = raw_weights.min()\n",
    "        max_weight = raw_weights.max()\n",
    "        normalized_weights = (raw_weights - min_weight) / (max_weight - min_weight)\n",
    "    else:\n",
    "        normalized_weights = torch.tensor(raw_weights, dtype=torch.float) if len(raw_weights) > 0 else None\n",
    "\n",
    "    # Create tensors for graph data\n",
    "    if edges:\n",
    "        edge_index = torch.tensor(edges, dtype=torch.long).t().contiguous()\n",
    "    else:\n",
    "        edge_index = torch.empty((2,0), dtype=torch.long)\n",
    "\n",
    "    if isinstance(centroids, np.ndarray):\n",
    "        x = torch.tensor(centroids, dtype=torch.float)\n",
    "    else:\n",
    "        x = torch.FloatTensor(centroids)\n",
    "\n",
    "    graph = Data(x=x, edge_index=edge_index, edge_attr=normalized_weights)\n",
    "\n",
    "    return graph"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 3.: Train the Graph Neural Network on the new representation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch_geometric.loader import DataLoader\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "def create_dataloaders(data, labels, batch_size=16, test_size=0.2, random_state=42):\n",
    "    \"\"\"\n",
    "    Creates PyTorch Geometric DataLoaders from a dataset.\n",
    "\n",
    "    Parameters:\n",
    "    - data (list of tuples): Each tuple contains (centroids, cluster_sizes).\n",
    "    - labels (list of int): Corresponding binary labels (0 or 1) for each graph.\n",
    "    - batch_size (int): Batch size for the DataLoader.\n",
    "    - test_size (float): Fraction of data to be used for validation.\n",
    "    - random_state (int): Seed for reproducibility.\n",
    "\n",
    "    Returns:\n",
    "    - train_loader (DataLoader): DataLoader for the training set.\n",
    "    - val_loader (DataLoader): DataLoader for the validation set.\n",
    "    \"\"\"\n",
    "    graphs = []\n",
    "    \n",
    "    # Create graphs using the construct_graph_for_gnn function\n",
    "    for input_data, label in zip(data, labels):\n",
    "        graph = construct_graph_for_gnn(input_data)\n",
    "        graph.y = torch.tensor([label], dtype=torch.float)  # Add label to the graph\n",
    "        graphs.append(graph)\n",
    "    \n",
    "    # Split dataset into training and validation sets\n",
    "    train_graphs, val_graphs = train_test_split(graphs, test_size=test_size, random_state=random_state)\n",
    "    \n",
    "    # Create DataLoaders\n",
    "    train_loader = DataLoader(train_graphs, batch_size=batch_size, shuffle=True)\n",
    "    val_loader = DataLoader(val_graphs, batch_size=batch_size)\n",
    "    \n",
    "    return train_loader, val_loader\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from torch_geometric.nn import GCNConv, global_mean_pool\n",
    "from torch.nn import BCEWithLogitsLoss\n",
    "from torch.optim import Adam\n",
    "from tqdm import tqdm\n",
    "\n",
    "\n",
    "def create_dataloaders(data, labels, batch_size=16, test_size=0.2, random_state=42):\n",
    "    \"\"\"\n",
    "    Creates PyTorch Geometric DataLoaders from a dataset.\n",
    "    \"\"\"\n",
    "    graphs = []\n",
    "    \n",
    "    # Convert data and labels to lists if they are Pandas series\n",
    "    data_list = list(data)\n",
    "    labels_list = list(labels)\n",
    "\n",
    "    # Create graphs using the construct_graph_for_gnn function\n",
    "    for input_data, label in zip(data_list, labels_list):\n",
    "        graph = construct_graph_for_gnn(input_data)\n",
    "        # Skip graphs with no clusters\n",
    "        if graph is None or graph.num_nodes == 0:\n",
    "            continue\n",
    "        \n",
    "        # Add label to the graph\n",
    "        # graph.y will be shape [1], after batching [batch_size]\n",
    "        graph.y = torch.tensor([label], dtype=torch.float)\n",
    "        graphs.append(graph)\n",
    "    \n",
    "    # Split dataset into training and validation sets\n",
    "    train_graphs, val_graphs = train_test_split(graphs, test_size=test_size, random_state=random_state)\n",
    "    \n",
    "    # Create DataLoaders\n",
    "    train_loader = DataLoader(train_graphs, batch_size=batch_size, shuffle=True)\n",
    "    val_loader = DataLoader(val_graphs, batch_size=batch_size)\n",
    "\n",
    "    return train_loader, val_loader\n",
    "\n",
    "\n",
    "class GraphClassifier(torch.nn.Module):\n",
    "    def __init__(self, in_channels, hidden_channels, num_classes=1):\n",
    "        super(GraphClassifier, self).__init__()\n",
    "        self.conv1 = GCNConv(in_channels, hidden_channels)\n",
    "        self.conv2 = GCNConv(hidden_channels, hidden_channels)\n",
    "        self.lin1 = torch.nn.Linear(hidden_channels, 100)\n",
    "        self.lin2 = torch.nn.Linear(100, num_classes)\n",
    "\n",
    "\n",
    "    def forward(self, data):\n",
    "        x, edge_index, edge_attr, batch = data.x, data.edge_index, data.edge_attr, data.batch\n",
    "        \n",
    "        x = self.conv1(x, edge_index, edge_weight=edge_attr)\n",
    "        x = F.relu(x)\n",
    "        x = self.conv2(x, edge_index, edge_weight=edge_attr)\n",
    "        x = F.relu(x)\n",
    "\n",
    "        # Global pooling\n",
    "        x = global_mean_pool(x, batch)  # [batch_size, hidden_channels]\n",
    "\n",
    "        # Output layer\n",
    "        x = self.lin1(x)\n",
    "        x = F.relu(x)\n",
    "        x = F.dropout(x, p=0.5, training=self.training)\n",
    "        x = self.lin2(x)\n",
    "\n",
    "\n",
    "def train(model, train_loader, val_loader, epochs, learning_rate):\n",
    "    device = torch.device(\"mps\") if torch.backends.mps.is_available() else torch.device(\"cpu\")\n",
    "    model.to(device)\n",
    "\n",
    "    criterion = BCEWithLogitsLoss()\n",
    "    optimizer = Adam(model.parameters(), lr=learning_rate)\n",
    "\n",
    "    training_losses = []\n",
    "    validation_losses = []\n",
    "    training_accuracies = []\n",
    "    validation_accuracies = []\n",
    "\n",
    "    for epoch in tqdm(range(epochs), total = epochs, desc=\"Epochs: \", unit='epoch'):\n",
    "        model.train()\n",
    "        train_loss = 0.0\n",
    "        correct = 0\n",
    "        total = 0\n",
    "\n",
    "        for batch in train_loader:\n",
    "            batch = batch.to(device)\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            out = model(batch).squeeze()  # out: [batch_size]\n",
    "            loss = criterion(out, batch.y)  # batch.y: [batch_size]\n",
    "\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            train_loss += loss.item()\n",
    "            \n",
    "            preds = (torch.sigmoid(out) > 0.5).float()\n",
    "            correct += (preds == batch.y).sum().item()\n",
    "            total += batch.y.size(0)\n",
    "\n",
    "        train_loss /= len(train_loader)\n",
    "        train_accuracy = correct / total\n",
    "        training_losses.append(train_loss)\n",
    "        training_accuracies.append(train_accuracy)\n",
    "\n",
    "        # Validation\n",
    "        model.eval()\n",
    "        val_loss = 0.0\n",
    "        correct = 0\n",
    "        total = 0\n",
    "        with torch.no_grad():\n",
    "            for batch in val_loader:\n",
    "                batch = batch.to(device)\n",
    "                out = model(batch).squeeze()\n",
    "                loss = criterion(out, batch.y)\n",
    "                val_loss += loss.item()\n",
    "\n",
    "                preds = (torch.sigmoid(out) > 0.5).float()\n",
    "                correct += (preds == batch.y).sum().item()\n",
    "                total += batch.y.size(0)\n",
    "\n",
    "        val_loss /= len(val_loader)\n",
    "        val_accuracy = correct / total\n",
    "        validation_losses.append(val_loss)\n",
    "        validation_accuracies.append(val_accuracy)\n",
    "\n",
    "        #if (epoch + 1) % 10 == 0:\n",
    "        print(f\"Epoch {epoch + 1}/{epochs}: \"\n",
    "                  f\"Train Loss = {train_loss:.4f}, Train Accuracy = {train_accuracy:.4f}, \"\n",
    "                  f\"Val Loss = {val_loss:.4f}, Val Accuracy = {val_accuracy:.4f}\")\n",
    "\n",
    "    return model, training_losses, validation_losses, training_accuracies, validation_accuracies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize model\n",
    "model = GraphClassifier(in_channels=768, hidden_channels=256, num_classes=1)\n",
    "\n",
    "train_loader, val_loader = create_dataloaders(train_data[\"Clusters\"], train_data[\"EventType\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trained_model, training_losses, validation_losses, training_accuracies, validation_accuracies = train(model, train_loader, val_loader, epochs=6, learning_rate=0.01)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Plot training and validation losses\n",
    "plt.figure(figsize=(12, 6))\n",
    "\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.plot(training_losses, label='Training Loss')\n",
    "plt.plot(validation_losses, label='Validation Loss')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Loss')\n",
    "plt.title('Training and Validation Losses')\n",
    "plt.legend()\n",
    "\n",
    "# Plot training and validation accuracies\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.plot(training_accuracies, label='Training Accuracy')\n",
    "plt.plot(validation_accuracies, label='Validation Accuracy')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.title('Training and Validation Accuracies')\n",
    "plt.legend()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 4.: Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def infer(model, data_loader, threshold=0.5, device=None):\n",
    "    \"\"\"\n",
    "    Perform inference using the trained GraphClassifier model.\n",
    "\n",
    "    Parameters:\n",
    "    - model (torch.nn.Module): Trained GraphClassifier model.\n",
    "    - data_loader (torch.utils.data.DataLoader): DataLoader containing the test or inference data.\n",
    "    - threshold (float): Threshold for binary classification. Default is 0.5.\n",
    "    - device (torch.device): Device to run inference on. Defaults to 'cpu'.\n",
    "\n",
    "    Returns:\n",
    "    - predictions (list): Predicted binary labels for each graph.\n",
    "    - probabilities (list): Predicted probabilities for each graph.\n",
    "    \"\"\"\n",
    "    if device is None:\n",
    "        device = torch.device(\"cpu\")\n",
    "\n",
    "    model.eval()  # Set model to evaluation mode\n",
    "    model.to(device)\n",
    "\n",
    "    predictions = []\n",
    "\n",
    "    with torch.no_grad():  # Disable gradient computation for inference\n",
    "        for batch in data_loader:\n",
    "            batch = batch.to(device)\n",
    "            logits = model(batch)  # Forward pass\n",
    "            probs = torch.sigmoid(logits).squeeze()  # Apply sigmoid for probabilities\n",
    "\n",
    "            # Convert probabilities to binary predictions\n",
    "            preds = (probs > threshold).long()\n",
    "\n",
    "\n",
    "            predictions.extend(preds.cpu().numpy())\n",
    "\n",
    "    return predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "eval_data = pd.DataFrame([])\n",
    "\n",
    "for file in tqdm(os.listdir('processed_data/eval_data')):\n",
    "    if file.endswith('.pkl'):\n",
    "        df = pd.read_pickle(f'processed_data/eval_data/{file}')\n",
    "        eval_data = pd.concat([eval_data, df], ignore_index=True)\n",
    "        del df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tqdm.pandas()\n",
    "eval_data[\"Clusters\"] = eval_data[\"Embeddings\"].progress_apply(cluster_embeddings_hdbscan)\n",
    "eval_data.drop(columns=[\"Embeddings\"], inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "eval_loader = DataLoader([construct_graph_for_gnn(item) for item in eval_data[\"Clusters\"].values], batch_size=16)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_clusters = eval_data[\"Clusters\"].apply(lambda x: len(x[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "eval_data[n_clusters == 0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = infer(trained_model, eval_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "submission = pd.DataFrame({\"ID\": eval_data[\"ID\"], \"EventType\": predictions})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "submission.to_csv(\"model_output/submissions/sub_4/submission.csv\", index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nlp_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
