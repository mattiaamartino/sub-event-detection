{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0235fb83-4f37-435f-a928-4b147b8600e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from textblob import TextBlob\n",
    "import symspellpy\n",
    "from symspellpy import SymSpell, Verbosity\n",
    "import os\n",
    "import pandas as pd\n",
    "import re\n",
    "from tqdm import tqdm\n",
    "import emoji\n",
    "from sentence_transformers import SentenceTransformer\n",
    "\n",
    "sym_spell = SymSpell(max_dictionary_edit_distance=2, prefix_length=7)\n",
    "dictionary_path = \"dictionary_path\" \n",
    "sym_spell.load_dictionary(dictionary_path, term_index=0, count_index=1)\n",
    "\n",
    "slang_dict = {\n",
    "    \"ur\": \"your\",\n",
    "    \"lol\": \"laughing out loud\",\n",
    "    \"brb\": \"be right back\",\n",
    "    \"idk\": \"I don't know\",\n",
    "    \"smh\": \"shaking my head\",\n",
    "    \"ht\": \"half time\", \n",
    "    \"ft\": \"full time\"\n",
    "}\n",
    "\n",
    "def expand_slang(text):\n",
    "    \"\"\"Expands common slang and acronyms.\"\"\"\n",
    "    words = text.split()\n",
    "    expanded_words = [slang_dict.get(word, word) for word in words]\n",
    "    return ' '.join(expanded_words)\n",
    "\n",
    "def correct_spelling(text):\n",
    "    \"\"\"Corrects spelling using SymSpell.\"\"\"\n",
    "    corrected_text = []\n",
    "    for word in text.split():\n",
    "        suggestions = sym_spell.lookup(word, Verbosity.CLOSEST, max_edit_distance=2)\n",
    "        if suggestions:\n",
    "            corrected_text.append(suggestions[0].term)\n",
    "        else:\n",
    "            corrected_text.append(word)\n",
    "    return ' '.join(corrected_text)\n",
    "\n",
    "def remove_emojis(text):\n",
    "    \"\"\"Removes emojis from text.\"\"\"\n",
    "    return emoji.replace_emoji(text, replace=\"\")  # Replaces emojis with an empty string\n",
    "\n",
    "def normalize_repeated_characters(text):\n",
    "    \"\"\"Normalizes excessive punctuation and repeated characters.\"\"\"\n",
    "    return re.sub(r'(.)\\1{2,}', r'\\1', text)\n",
    "\n",
    "def transform_hashtags(text):\n",
    "    \"\"\"Transforms hashtags into readable text.\"\"\"\n",
    "    hashtags = re.findall(r'#(\\w+)', text)\n",
    "    for hashtag in hashtags:\n",
    "        words = hashtag.split('_')\n",
    "        separated_words = []\n",
    "        for word in words:\n",
    "            if word.isupper():\n",
    "                separated_words.append(word)\n",
    "            else:\n",
    "                split_words = re.findall('[A-Z][^A-Z]*', word)\n",
    "                if split_words:\n",
    "                    separated_words.extend(split_words)\n",
    "                else:\n",
    "                    separated_words.append(word)\n",
    "        clean_text = ' '.join(separated_words).lower()\n",
    "        text = text.replace(f'#{hashtag}', clean_text)\n",
    "    return text\n",
    "\n",
    "def clean_and_process_tweets(input_folder, output_folder):\n",
    "    \"\"\"Preprocess tweets in all CSV files in the input folder.\"\"\"\n",
    "    os.makedirs(output_folder, exist_ok=True)\n",
    "    phrases_to_remove = [\n",
    "        r'follow & rt to enter!?\\.?', \n",
    "        r'rt & follow to enter!?\\.?' \n",
    "    ]\n",
    "    \n",
    "    for file_name in tqdm(os.listdir(input_folder), desc='Cleaning and processing tweets', unit='files'):\n",
    "        if file_name.endswith('.csv'):\n",
    "            input_file_path = os.path.join(input_folder, file_name)\n",
    "            output_file_path = os.path.join(output_folder, file_name)\n",
    "            df = pd.read_csv(input_file_path)\n",
    "            df = df.drop_duplicates()  # Drop duplicate rows\n",
    "            \n",
    "            if 'Tweet' in df.columns:\n",
    "                df = df.drop_duplicates(subset='Tweet')  # Drop duplicate tweets\n",
    "                df['Tweet'] = df['Tweet'].apply(lambda x: re.sub(r'http\\S+|www\\S+|https\\S+|httâ€¦', '', str(x)))  # Remove links\n",
    "                df['Tweet'] = df['Tweet'].apply(lambda x: re.sub(r'^RT\\s+@\\w+:\\s+', '', str(x)))  # Remove RT\n",
    "                df['Tweet'] = df['Tweet'].apply(lambda x: re.sub(r'@\\w+', 'user', str(x)))  # Replace @usernames\n",
    "                df['Tweet'] = df['Tweet'].apply(transform_hashtags)  # Transform hashtags\n",
    "                df['Tweet'] = df['Tweet'].apply(lambda x: re.sub(r'#', '', str(x)))  # Remove leftover hashtags\n",
    "                df['Tweet'] = df['Tweet'].apply(lambda x: str(x).lower())  # Convert to lowercase\n",
    "                df['Tweet'] = df['Tweet'].apply(remove_emojis)  # Remove emojis\n",
    "                df['Tweet'] = df['Tweet'].apply(normalize_repeated_characters)  # Normalize repeated characters\n",
    "                df['Tweet'] = df['Tweet'].apply(expand_slang)  # Expand slang\n",
    "                df['Tweet'] = df['Tweet'].apply(correct_spelling)  # Correct spelling\n",
    "                for phrase in phrases_to_remove:\n",
    "                    df['Tweet'] = df['Tweet'].apply(lambda x: re.sub(phrase, '', str(x), flags=re.IGNORECASE))  # Remove phrases\n",
    "                df['Tweet'] = df['Tweet'].apply(lambda x: re.sub(r'^:\\s*', '', str(x)))  # Remove leading colons\n",
    "                df['Tweet'] = df['Tweet'].apply(lambda x: re.sub(r'\\s+', ' ', str(x)))  # Remove extra spaces\n",
    "                df['Tweet'] = df['Tweet'].apply(lambda x: x.strip())  # Remove leading/trailing spaces\n",
    "                df['Tweet'] = df['Tweet'].apply(lambda x: x.replace('\\n', ' '))  # Replace newlines with spaces\n",
    "            \n",
    "            df.to_csv(output_file_path, index=False)  # Save preprocessed data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7478a3d4-0774-4de7-90e5-bab6bf5ac158",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_input_folder = \"input train\"\n",
    "eval_input_folder = \"eval train\"\n",
    "train_output_folder = \"train output\"\n",
    "eval_output_folder = \"eval output\"\n",
    "print(\"Preprocessing train and eval data...\")\n",
    "clean_and_process_tweets(train_input_folder, train_output_folder)\n",
    "clean_and_process_tweets(eval_input_folder, eval_output_folder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "474b1e18-0235-49ef-96ce-d47558c64a15",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = SentenceTransformer('all-MiniLM-L6-v2')  \n",
    "\n",
    "def generate_embeddings(data, column_name=\"Tweet\"):\n",
    "    \"\"\"\n",
    "    Generate embeddings for the specified column in a DataFrame.\n",
    "\n",
    "    Args:\n",
    "        data (pd.DataFrame): The input DataFrame.\n",
    "        column_name (str): The name of the column containing text data.\n",
    "\n",
    "    Returns:\n",
    "        pd.DataFrame: The DataFrame with a new 'Embedding' column containing embeddings.\n",
    "    \"\"\"\n",
    "    print(f\"Generating embeddings for {column_name}...\")\n",
    "    data['Embedding'] = list(bert_model.encode(data[column_name].tolist(), show_progress_bar=True))\n",
    "    return data\n",
    "\n",
    "def load_csv_folder(folder_path):\n",
    "    \"\"\"\n",
    "    Load and combine all CSV files from a given folder into a single DataFrame.\n",
    "\n",
    "    Args:\n",
    "        folder_path (str): Path to the folder containing CSV files.\n",
    "\n",
    "    Returns:\n",
    "        pd.DataFrame: Combined DataFrame.\n",
    "    \"\"\"\n",
    "    dataframes = []\n",
    "    for filename in tqdm(os.listdir(folder_path), desc=f\"Loading files from {folder_path}\"):\n",
    "        if filename.endswith(\".csv\"):\n",
    "            file_path = os.path.join(folder_path, filename)\n",
    "            df = pd.read_csv(file_path)\n",
    "            dataframes.append(df)\n",
    "    return pd.concat(dataframes, ignore_index=True)\n",
    "\n",
    "train_data = load_csv_folder(\"/Users/jamilya/Desktop/challenge_data_neq/train_bert\")\n",
    "eval_data = load_csv_folder(\"/Users/jamilya/Desktop/challenge_data_neq/eval_tweets_bert\")\n",
    "\n",
    "train_data = generate_embeddings(train_data, column_name=\"Tweet\")\n",
    "eval_data = generate_embeddings(eval_data, column_name=\"Tweet\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88ec062c-8b9d-440f-b534-170514e14f93",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "def aggregate_embeddings(data, target_exists=True):\n",
    "    \"\"\"\n",
    "    Aggregate tweet embeddings by ID. If target_exists is False, skip aggregating EventType.\n",
    "    \"\"\"\n",
    "    if target_exists:\n",
    "        aggregated_data = data.groupby('ID').agg({\n",
    "            'Embedding': lambda x: np.mean(np.vstack(x), axis=0),  # Mean of embeddings for each ID\n",
    "            'EventType': 'first'  \n",
    "        }).reset_index()\n",
    "    else:\n",
    "        aggregated_data = data.groupby('ID').agg({\n",
    "            'Embedding': lambda x: np.mean(np.vstack(x), axis=0)  \n",
    "        }).reset_index()\n",
    "    return aggregated_data\n",
    "\n",
    "print(\"Aggregating embeddings by ID...\")\n",
    "train_data['Embedding'] = train_data['Embedding'].apply(np.array)\n",
    "eval_data['Embedding'] = eval_data['Embedding'].apply(np.array)\n",
    "\n",
    "train_aggregated = aggregate_embeddings(train_data, target_exists=True)\n",
    "eval_aggregated = aggregate_embeddings(eval_data, target_exists=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09417ac7-b608-4b62-b3d6-078c24cce5d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = pd.DataFrame(train_aggregated['Embedding'].tolist())\n",
    "y = train_aggregated['EventType']\n",
    "X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.2, random_state=42, stratify=y)\n",
    "X_eval = pd.DataFrame(eval_aggregated['Embedding'].tolist())\n",
    "print(\"Training LightGBM model...\")\n",
    "lgb_model = LGBMClassifier(random_state=42, n_estimators=100, learning_rate=0.1, max_depth=3)\n",
    "lgb_model.fit(X_train, y_train)\n",
    "\n",
    "y_val_pred = lgb_model.predict(X_val)\n",
    "validation_accuracy = accuracy_score(y_val, y_val_pred)\n",
    "print(f\"Validation Accuracy: {validation_accuracy:.4f}\")\n",
    "\n",
    "print(\"Predicting EventType for evaluation data...\")\n",
    "eval_aggregated['EventType'] = lgb_model.predict(X_eval)\n",
    "\n",
    "output_file = \"predicted_event_types_lightgbm_old.csv\"\n",
    "eval_aggregated[['ID', 'EventType']].to_csv(output_file, index=False)\n",
    "print(f\"Predictions saved to {output_file}\") "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
